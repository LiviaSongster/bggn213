---
title: "Class08 - Machine Learning 1"
author: "Livia Songster"
date: "10/25/2019"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## K-means example

We will make up some data to cluster.

```{r}
tmp <- c(rnorm(30,-3), rnorm(30,3))
x <- cbind(x=tmp, y=rev(tmp))

plot(x)
```

Use the kmeans() function setting k to 2 and nstart=20. Inspect/print the results.

```{r}
k <- kmeans(x,centers=2,nstart=20)
k
```
Q. How many points are in each cluster? *both are 30.*

Q. What ‘component’ of your result object details 
 - cluster size? *Shown in component 7 "size"*
 
 - cluster assignment/membership? *Shown in component 1 "cluster"*
 
 - cluster center? *shown in component 2 "centers"*

```{r}
# for size
k$size

# for centers
k$centers
```

Plot x colored by the kmeans cluster assignment and add cluster centers as blue points

```{r}
plot(x,col=k$cluster)
# this will add the cluster centers as blue points
points(k$center,col="blue",pch=15)
```

## Hierarchical clustering
```{r}
# First we need to calculate point (dis)similarity as the Euclidean distance between observations
dist_matrix <- dist(x)

# The hclust() function returns a hierarchical clustering model
hc <- hclust(d = dist_matrix)

# the print method is not so useful here
hc

# Our input is a distance matrix from the dist() function. Lets make sure we understand it first
dist_matrix <- dist(x)
dim(dist_matrix)

# view the matrix
# View( as.matrix(dist_matrix) )

# check dimensions of x
dim(x)

# check dimenstions of our distance matrix
dim(as.matrix(dist_matrix))

# Create hierarchical cluster model: hc
hc <- hclust(dist(x))

# We can plot the results as a dendrogram
plot(hc)

# Add a red line at height 6
abline(h=6, col="red")

# Cut by height h=6; prints which clusters each point will lie in
cutree(hc, h=6) 

# Cut into two groups; prints which clusters each point will lie in
cutree(hc, k=2) 
```

## Linking clusters in hierarchical clustering
How is distance between clusters determined?

There are four main methods to determine which cluster should be linked:

➡ *Complete*: pairwise similarity between all observations in cluster 1 and cluster 2, and uses largest of similarities
```{r}
hc.complete <- hclust(dist(x),method="complete")
plot(hc.complete,main="Complete cluster similarity")
```

➡ *Single*: same as above but uses smallest of similarities
```{r}
hc.single <- hclust(dist(x),method="single")
plot(hc.single,main="Single cluster similarity")
```

➡ *Average*: same as above but uses average of similarities
```{r}
hc.average <- hclust(dist(x),method="average")
plot(hc.average,main="Average cluster similarity")
```

➡ *Centroid*: finds centroid of cluster 1 and centroid of cluster 2, and uses similarity between two centroids 
```{r}
hc.centroid <- hclust(dist(x),method="centroid")
plot(hc.centroid,main="Centroid cluster similarity")
```


## More examples and practice

### Step 1. Generate some example data for clustering
```{r}
x <- rbind(
 matrix(rnorm(100, mean=0, sd = 0.3), ncol = 2), # c1
 matrix(rnorm(100, mean = 1, sd = 0.3), ncol = 2), # c2
 matrix(c(rnorm(50, mean = 1, sd = 0.3), rnorm(50, mean = 0, sd = 0.3)), ncol = 2)) # c3
colnames(x) <- c("x", "y")
head(x)
```

### Step 2. Plot the data without clustering
```{r}
plot(x)
```

### Step 3. Generate colors for known clusters (just so we can compare to hclust results)
```{r}
col <- as.factor( rep(c("c1","c2","c3"), each=50) )

plot(x, col=col,main="Known clusters")
```

### Your Turn!

Q. Use the dist(), hclust(), plot() and cutree() functions to return 2 and 3 clusters
```{r}
hc <- hclust(d = dist(x))
plot(hc,main="Hierarchical cluster dendrogram")

# 2 clusters
clust2 <- cutree(hc,k=2)
plot(x,col=clust2,main="2 Hierarchical clusters")
# 3 clusters
clust3 <- cutree(hc,k=3)
plot(x,col=clust3,main="3 Hierarchical clusters")

# See how many points went into each cluster
table(col,clust3)

```

Q. How does this compare to your known 'col' groups?
```{r}
# also do kmeans quick
k2 <- kmeans(x,centers=2,nstart=20)
k3 <- kmeans(x,centers=3,nstart=20)


# put plots into quadrant
par(mfrow=c(2,3))
plot(x, col=col,main="Known clusters")
plot(hc,main="Hierarchical cluster dendrogram")
plot(x,col=clust2,main="2 Hierarchical clusters")
plot(x,col=clust3,main="3 Hierarchical clusters")
plot(x,col=k2$cluster,main="K-means k = 2")
points(k2$center,col="blue",pch=15)

plot(x,col=k3$cluster,main="K-means k = 3")
points(k3$center,col="blue",pch=15)
```

# Introduction to PCA plots

```{r}
mydata <- read.csv("expression.csv",row.names = 1)
head(mydata) 
```

The samples are columns, and the genes are rows! Now we have our data, we call prcomp() to do PCA.

NOTE: prcomp() expects the samples to be rows and genes to be columns so we need to first transpose the matrix with the t() function.

```{r}
# first transpose the data 
pca <- prcomp(t(mydata), scale=TRUE)

## See what is returned by the prcomp() function
attributes(pca) 
```

The output for x contains the principal components (PCs) for drawing our first graph. Here we will take the first two columns in pca$x (corresponding to PC1 and PC2) to draw a 2-D plot.

```{r}
## A basic PC1 vs PC2 2-D plot
plot(pca$x[,1], pca$x[,2]) 
summary(pca)
```

Looks interesting with a nice separation of samples into two groups of 5 samples each. Now we can use the square of pca$sdev, which stands for “standard deviation”, to calculate how much variation in the original data each PC accounts for.

```{r}
## Variance captured per PC
pca.var <- pca$sdev^2

## Percent variance is often more informative to look at
pca.var.per <- round(pca.var/sum(pca.var)*100, 1)

pca.var.per

# next let's make a scree plot
barplot(pca.var.per, main="Scree Plot",
 xlab="Principal Component", ylab="Percent Variation")


## A vector of colors for wt and ko samples
colvec <- colnames(mydata)
colvec[grep("wt", colvec)] <- "red"
colvec[grep("ko", colvec)] <- "blue"

plot(pca$x[,1], pca$x[,2], col=colvec, pch=16,
 xlab=paste0("PC1 (", pca.var.per[1], "%)"),
 ylab=paste0("PC2 (", pca.var.per[2], "%)"))

text(pca$x[,1], pca$x[,2], labels = colnames(mydata), pos=c(rep(4,5), rep(2,5)))
## IN THE CONSOLE! Click to identify which sample is which
## identify(pca$x[,1], pca$x[,2], labels=colnames(mydata))
## Press ESC to exit… 


## Another way to color by sample type
## Extract the first 2 characters of the sample name
sample.type <- substr(colnames(mydata),1,2)
sample.type

## now use this as a factor input to color our plot
plot(pca$x[,1], pca$x[,2], col=as.factor(sample.type), pch=16)
```

Find the top 10 measurements (genes) that contribute most to pc1 in either direction (+ or -)
```{r}
loading_scores <- pca$rotation[,1]

## Find the top 10 measurements (genes) that contribute
## most to PC1 in either direction (+ or -)
gene_scores <- abs(loading_scores) 
gene_score_ranked <- sort(gene_scores, decreasing=TRUE)

## show the names of the top 10 genes
top_10_genes <- names(gene_score_ranked[1:10])
top_10_genes

## plot the loading scores
barplot(pca$rotation[,1], las=2 )

```

# Next let's analyze some food data from the UK

```{r}
x <- read.csv("UK_foods.csv",row.names = 1)

# check dimentions
dim(x)
head(x,6)

# next generate a bar plot to visualize the data... not very useful
par(mfrow=c(1,2))
barplot(as.matrix(x), beside=T, col=rainbow(nrow(x)),main="barplot",las=2)
# vertical stacked plot
barplot(as.matrix(x), beside=F, col=rainbow(nrow(x)),main="stacked barplot",las=2) # las=2 will make sideways x axis labels

```

Next let's try to make all pairwise plots. 
```{r}
pairs(x, col=rainbow(10), pch=16)
```
Now for some PCA to make this actually interpretable:
```{r}
# Use the prcomp() PCA function 
pca <- prcomp(t(x)) # transpose the data
summary(pca)

# scree plot
plot(pca,main="scree plot")

# Plot PC1 vs PC2
plot(pca$x[,1], pca$x[,2], xlab="PC1", ylab="PC2", xlim=c(-270,500))

text(pca$x[,1], pca$x[,2], colnames(x)) # this code adds the data name on top of the point (in light gray)

# next add color to the plot so the colors of the country names match the colors in the map and table

plot(pca$x[,1], pca$x[,2], xlab="PC1", ylab="PC2", xlim=c(-270,500),col=c("orange","pink","blue","green"))
text(pca$x[,1], pca$x[,2], colnames(x),col=c("orange","pink","blue","green"))

```

Calculate the variance in the original data that each PC accounts for.
```{r}
# all data
v <- round( pca$sdev^2/sum(pca$sdev^2) * 100 )
v

## or the second row here...
z <- summary(pca)
z$importance

# plot the % variances
barplot(v, xlab="Principal Component", ylab="Percent Variation")

```

## Next let's look at the loading scores (original variables of the PCs) from the rotation component of prcomp().
```{r}
## Lets focus on PC1 as it accounts for > 90% of variance 
par(mar=c(10, 3, 1, 0)) # change the margins
head(pca$rotation)
barplot( pca$rotation[,1], las=2,main="PC1 loading scores")

## and now PC2
barplot( pca$rotation[,2], las=2,main="PC2 loading scores")


```

# This can be summarized in a biplot.

```{r}
## The builtin biplot() can be useful for small datasets 
biplot(pca,main="BiPlot")
```
